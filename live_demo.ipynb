{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomas-chong/fine-tuning-gemma-with-unsloth/blob/main/live_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "### **Presented by Thomas Chong**\n",
        "*   **Google Developer Expert in AI**\n",
        "*   **AI Research Engineer at Beever AI**\n",
        "\n",
        "<a href=\"https://www.linkedin.com/in/chongcht/\" target=\"_blank\"><img src=\"https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white\" alt=\"LinkedIn\"></a>\n",
        "<a href=\"https://scholar.google.com/citations?user=aV-Ddp4AAAAJ&hl=en\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Google_Scholar-4285F4?style=for-the-badge&logo=google-scholar&logoColor=white\" alt=\"Google Scholar\"></a>\n",
        "<a href=\"https://github.com/thomas-chong\" target=\"_blank\"><img src=\"https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white\" alt=\"GitHub\"></a>\n",
        "<a href=\"https://thomas-chong.github.io/\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Website-464646?style=for-the-badge&logo=About.me&logoColor=white\" alt=\"Website\"></a>\n",
        "\n",
        "\n",
        "By day, I'm an AI Research Engineer at [Beever AI](https://beever.ai/), a new-established AI research subsidiary in Toronto, by [Votee AI](https://votee.ai/) in Hong Kong. My work involves a mix of in-house R&D for our agentic AI products and publishing research papers through collaborations with external partners.\n",
        "\n",
        "# Part 1: Introduction & Setup\n",
        "\n",
        "### **Live Model Fine-Tuning: Customizing Gemma 3 270M in Under 30 Minutes**\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"https://raw.githubusercontent.com/thomas-chong/fine-tuning-gemma-with-unsloth/main/images/devfest_waterloo.png\" alt=\"DevFest Waterloo\" style=\"width: 100%; max-width: 600px;\">\n",
        "</div>\n",
        "\n",
        "This session provides a comprehensive, live demonstration of a complete fine-tuning workflow for Google's Gemma 3 270M model, executed in under 30 minutes. The primary objective is to illustrate how recent advancements in model architecture, training techniques, and software optimization have made customizing powerful Large Language Models (LLMs) accessible to a broader audience, even in resource-constrained environments.\n",
        "\n",
        "The traditional approach, full fine-tuning, involves updating every single parameter in the model. This process is computationally intensive, requiring significant GPU memory and extended training times. This notebook will showcase a modern, efficient alternative by strategically combining three key technologies:\n",
        "\n",
        "1.  **Gemma 3 270M**: A compact, state-of-the-art open model from Google. Its small footprint makes it an ideal candidate for rapid experimentation and deployment on consumer-grade hardware.\n",
        "2.  **QLoRA (Quantized Low-Rank Adaptation)**: A highly efficient fine-tuning method. QLoRA dramatically reduces memory by quantizing the model's weights to 4-bits and then inserting small, trainable \"adapter\" layers. We only update these adapters, leaving the millions of base model parameters frozen.\n",
        "3.  **Unsloth**: An open-source library with optimized CUDA kernels that can make fine-tuning up to 2x faster while reducing memory usage by 60-70%. Unsloth is the critical enabler that makes this sub-30-minute demonstration feasible on a free Google Colab GPU.\n",
        "\n",
        "This rapid fine-tuning is possible due to the convergence of these three elements: a capable base model, a memory-frugal training technique, and a speed-optimized library.\n",
        "\n",
        "**Agenda:**\n",
        "1.  **Prerequisites:** What are Fine-Tuning and LoRA?\n",
        "2.  **Setup & Installation:** Getting our environment ready.\n",
        "3.  **Data Generation:** Creating a synthetic dataset with the Gemini Batch API.\n",
        "4.  **Live Fine-Tuning:** Loading, configuring, and training our model.\n",
        "5.  **Evaluation:** Was it worth it? A side-by-side comparison with an LLM-as-a-Judge.\n",
        "6.  **Conclusion & Resources.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Overall Fine-Tuning Pipeline\n",
        "\n",
        "This diagram illustrates the end-to-end workflow we will follow in this notebook, from generating a synthetic dataset to exporting a locally-runnable model.\n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "    subgraph \"Part 1: Data Generation\"\n",
        "        A[Start: Define Creative Themes] --> B{Gemini Batch API};\n",
        "        B --> C[Generate Synthetic Dataset];\n",
        "    end\n",
        "\n",
        "    subgraph \"Part 2: Model Training\"\n",
        "        C --> D[Load Base Gemma 3 Model];\n",
        "        D --> E[Inject QLoRA Adapters];\n",
        "        E --> F(Fine-Tune with Unsloth);\n",
        "    end\n",
        "\n",
        "    subgraph \"Part 3: Evaluation & Export\"\n",
        "        F --> G{A/B Test: Base vs. Fine-Tuned};\n",
        "        G --> H[LLM-as-a-Judge Verdict];\n",
        "        H --> I[Save LoRA Adapters];\n",
        "        I --> J[Export to GGUF for Local Inference];\n",
        "        J --> K[End: Ready for LM Studio];\n",
        "    end\n",
        "\n",
        "    style F fill:#a2de89,stroke:#333,stroke-width:2px\n",
        "    style J fill:#f9d77f,stroke:#333,stroke-width:2px\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What is Fine-Tuning?\n",
        "\n",
        "Fine-tuning adapts a general pre-trained model to a specific task using a smaller, focused dataset. It's like teaching a polymath to become a specialist in a particular field.\n",
        "\n",
        "### What is LoRA (Low-Rank Adaptation)?\n",
        "\n",
        "Full fine-tuning requires updating all model weights, which is computationally expensive. **LoRA** offers a much more efficient alternative. It freezes the original weights and injects small, trainable \"adapter\" matrices (A and B) into the model. We only train these adapters, which are a tiny fraction of the total parameters.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"https://raw.githubusercontent.com/thomas-chong/fine-tuning-gemma-with-unsloth/main/images/what_is_lora.png\" alt=\"LoRA Diagram\" style=\"width: 100%; max-width: 800px;\">\n",
        "</div>\n",
        "\n",
        "This makes fine-tuning incredibly fast and memory-efficient. But does it compromise on quality?\n",
        "\n",
        "> #### **Key Insight from \"LoRA Without Regret\"**\n",
        ">\n",
        "> Research shows that LoRA can match the performance of full fine-tuning if configured correctly. One of the most critical, and often overlooked, conditions is **applying LoRA to all possible layers.**\n",
        ">\n",
        "> From the paper: *\"Attention-only LoRA significantly underperforms MLP-only LoRA, and does not further improve performance on top of LoRA-on-MLP.\"*\n",
        ">\n",
        "> This is because the MLP (feed-forward) layers contain a vast number of parameters compared to the attention layers. By ignoring them, you're leaving most of the model's knowledge untapped and creating a bottleneck for learning. **Therefore, we will target all linear layers in our configuration.**\n",
        ">\n",
        "> Link to the blog post: [LoRA Without Regret](https://thinkingmachines.ai/blog/lora/)\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"https://raw.githubusercontent.com/thomas-chong/fine-tuning-gemma-with-unsloth/main/images/lora_without_regret.png\" alt=\"LoRA Diagram\" style=\"width: 100%; max-width: 800px;\">\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Dependencies\n",
        "\n",
        "Install Unsloth from GitHub for the latest patches and pin other libraries for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.55.4\n",
        "!pip install --no-deps trl==0.22.2\n",
        "\n",
        "# Install the Google GenAI SDK for using the Gemini API.\n",
        "%pip install -q -U \"google-genai\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure API Keys\n",
        "\n",
        "To run this notebook, you'll need API keys for both Google Gemini and the Hugging Face Hub. For security, we'll use Colab's built-in secret manager.\n",
        "\n",
        "1.  Click on the **key icon** (ðŸ”‘) in the left-hand sidebar.\n",
        "2.  Create two new secrets:\n",
        "    *   **Name:** `GOOGLE_API_KEY` -> **Value:** Your Gemini API Key\n",
        "    *   **Name:** `HF_TOKEN` -> **Value:** Your Hugging Face access token (with `write` permissions)\n",
        "\n",
        "Once you've added these secrets, the notebook will be able to access them securely.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://raw.githubusercontent.com/thomas-chong/fine-tuning-gemma-with-unsloth/main/images/secret_settings.png\" width=\"600\">\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title  Configurations\n",
        "#@markdown ---\n",
        "#@markdown ### **Model & Dataset**\n",
        "model_name = \"unsloth/gemma-3-270m-it\" #@param {type:\"string\"}\n",
        "max_seq_length = 2048 #@param {type:\"integer\"}\n",
        "use_pregenerated_dataset = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown ### **LoRA Parameters**\n",
        "lora_r = 16 #@param {type:\"slider\", min:4, max:64, step:4}\n",
        "#@markdown ---\n",
        "#@markdown ### **Training Parameters**\n",
        "max_steps = 150 #@param {type:\"integer\"}\n",
        "learning_rate = 2e-4 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "#@markdown ### **Hugging Face**\n",
        "hf_user = \"your-username\" #@param {type:\"string\"}\n",
        "#@markdown ---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Import Libraries & Configure Secrets\n",
        "# Core ML and Unsloth imports\n",
        "from unsloth import FastModel\n",
        "import torch\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Google Gemini API imports\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import json\n",
        "import time\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get API keys from Colab secrets\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "google_api_key = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Data is Fuel - Synthetic Data Generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bootstrapping Our Dataset with the Gemini Batch API\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"https://raw.githubusercontent.com/thomas-chong/fine-tuning-gemma-with-unsloth/main/images/gemini_batch_api.png\" alt=\"Gemini Batch API\" style=\"width: 100%; max-width: 800px;\">\n",
        "</div>\n",
        "\n",
        "A common challenge in fine-tuning is the \"cold start\" problem, where you have a task in mind but lack labeled training examples. **Synthetic data generation** offers a powerful solution, allowing us to use a highly capable LLM to create a labeled dataset from scratch.\n",
        "\n",
        "This section demonstrates how to use the **Gemini Batch API** to generate our creative writing dataset. This API is specifically designed for high-volume, asynchronous tasks. It processes large numbers of requests in parallel at a 50% cost discount compared to its real-time counterpart, making it an ideal tool for large-scale data generation.\n",
        "\n",
        "The workflow is as follows:\n",
        "1. A prompt template is designed to instruct the Gemini model to generate creative stories.\n",
        "2. These prompts are formatted into a JSON Lines (JSONL) file.\n",
        "3. The file is submitted as an asynchronous batch job.\n",
        "\n",
        "Since batch jobs are not instantaneous, this demonstration will show the code for submitting the job and then proceed with a pre-generated set of results to maintain the pace of the live session. This approach showcases a sophisticated data augmentation strategy that enhances the model's generalization capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- CONFIGURATION ---\n",
        "HF_DATASET_NAME = f\"{hf_user}/synthetic-creative-writing\"\n",
        "\n",
        "if not use_pregenerated_dataset:\n",
        "    from datasets import Dataset\n",
        "    # --- Gemini API Setup ---\n",
        "    # Initialize the client for Batch API usage\n",
        "    client = genai.Client(api_key=google_api_key)\n",
        "\n",
        "    # --- Data Generation Pipeline ---\n",
        "    SYSTEM_PROMPT = \"You are a master storyteller. Write a short, imaginative story based on the user's request. The story should be concise and suitable for a general audience. The story should not exceed 2048 tokens.\"\n",
        "\n",
        "    STORY_THEMES = [\n",
        "        \"A robot who discovers music for the first time.\",\n",
        "        \"A magical library where books come to life.\",\n",
        "        \"A detective who solves crimes in a city powered by steam.\",\n",
        "        \"Two pen pals from different planets meeting for the first time.\",\n",
        "        \"A mischievous forest spirit who plays pranks on hikers.\",\n",
        "        \"The last dragon on Earth sharing its wisdom with a young child.\",\n",
        "    ]\n",
        "\n",
        "    def create_batch_requests(themes, system_prompt):\n",
        "        \"\"\"Creates a JSONL file for the Gemini Batch API.\"\"\"\n",
        "        jsonl_file_path = 'synthetic_story_requests.jsonl'\n",
        "        with open(jsonl_file_path, 'w') as f:\n",
        "            for i, theme in enumerate(themes):\n",
        "                request = {\n",
        "                    \"key\": f\"request_{i}\",\n",
        "                    \"request\": {\n",
        "                         \"systemInstruction\": {\n",
        "                            \"parts\": [{\"text\": system_prompt}]\n",
        "                        },\n",
        "                        \"contents\": [\n",
        "                           {\"parts\": [{\"text\": theme}]}\n",
        "                        ],\n",
        "                        \"generationConfig\": {\n",
        "                            \"maxOutputTokens\": 2048,\n",
        "                            \"temperature\": 0.9,\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "                f.write(json.dumps(request) + '\\n')\n",
        "        return jsonl_file_path\n",
        "\n",
        "    # Create and run the batch job\n",
        "    print(\"Creating batch requests file...\")\n",
        "    requests_file = create_batch_requests(STORY_THEMES * 50, SYSTEM_PROMPT) # Create 300 examples\n",
        "    print(f\"Uploading file: {requests_file}\")\n",
        "    uploaded_file = client.files.upload(\n",
        "        file=requests_file,\n",
        "        config=types.UploadFileConfig(display_name='my-batch-requests', mime_type='jsonl')\n",
        "    )\n",
        "    batch_job = client.batches.create(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        src=uploaded_file.name,\n",
        "    )\n",
        "    print(f\"Batch job created: {batch_job.name}. Polling for results...\")\n",
        "\n",
        "    # Polling logic\n",
        "    while True:\n",
        "        batch_job = client.batches.get(name=batch_job.name)\n",
        "        if batch_job.state.name in ('JOB_STATE_SUCCEEDED', 'JOB_STATE_FAILED', 'JOB_STATE_CANCELLED'):\n",
        "            break\n",
        "        print(f\"Job not finished. Current state: {batch_job.state.name}. Waiting 30 seconds...\")\n",
        "        time.sleep(30)\n",
        "\n",
        "    print(f\"Job finished with state: {batch_job.state.name}\")\n",
        "\n",
        "    if batch_job.state.name == 'JOB_STATE_SUCCEEDED':\n",
        "        results = []\n",
        "        result_file_name = batch_job.dest.file_name\n",
        "        print(f\"Results are in file: {result_file_name}\")\n",
        "\n",
        "        file_content_bytes = client.files.download(file=result_file_name)\n",
        "        file_content = file_content_bytes.decode('utf-8')\n",
        "\n",
        "        print(\"Parsing results...\")\n",
        "        for line in file_content.splitlines():\n",
        "            if line:\n",
        "                parsed_response = json.loads(line)\n",
        "                original_request_key = parsed_response.get('key', '')\n",
        "                try:\n",
        "                    request_index = int(original_request_key.split('_')[1])\n",
        "                    prompt = STORY_THEMES[request_index % len(STORY_THEMES)]\n",
        "                except (IndexError, ValueError):\n",
        "                    prompt = \"Unknown prompt\"\n",
        "\n",
        "                response_text = \"\"\n",
        "                try:\n",
        "                    response_text = parsed_response['response']['candidates'][0]['content']['parts'][0]['text']\n",
        "                except (KeyError, IndexError):\n",
        "                    response_text = \"Error: Could not parse response.\"\n",
        "\n",
        "                results.append({'prompt': prompt, 'response': response_text})\n",
        "\n",
        "        # Create a Hugging Face dataset\n",
        "        dataset = Dataset.from_list(results)\n",
        "\n",
        "        # Optional: Push to Hub\n",
        "        if hf_user != \"your-username\":\n",
        "            try:\n",
        "                print(f\"Pushing dataset to Hugging Face Hub: {HF_DATASET_NAME}\")\n",
        "                dataset.push_to_hub(HF_DATASET_NAME, private=True, token=hf_token)\n",
        "                print(\"Dataset pushed successfully.\")\n",
        "                use_pregenerated_dataset = True\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to push dataset to Hub: {e}\")\n",
        "                print(\"Proceeding with local dataset.\")\n",
        "        else:\n",
        "            print(\"hf_user is not set. Skipping push to Hub and proceeding with local dataset.\")\n",
        "\n",
        "    else:\n",
        "        if batch_job.state.name == 'JOB_STATE_FAILED':\n",
        "            print(f\"Job failed. Error: {batch_job.error}\")\n",
        "        else:\n",
        "            print(f\"Job did not succeed (State: {batch_job.state.name}).\")\n",
        "\n",
        "        # Fallback to pregenerated dataset if generation fails\n",
        "        print(\"Falling back to pre-generated dataset.\")\n",
        "        use_pregenerated_dataset = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: Live Fine-Tuning with Unsloth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True, # Use QLoRA for memory efficiency\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Formatting for Gemma 3\n",
        "\n",
        "Instruction-tuned models like Gemma 3 are pre-trained on data formatted with specific \"chat templates.\" These templates use special tokens to delineate conversational turns (e.g., from a user and a model). Adhering to this format during fine-tuning is critical for the model to correctly interpret the task and generate responses in the desired style.\n",
        "\n",
        "The Gemma 3 chat template follows this structure:\n",
        "\n",
        "`<bos><start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n{response}<end_of_turn><eos>`\n",
        "\n",
        "Failing to use the correct template introduces a **distribution shift** between our fine-tuning data and the model's original training data. This mismatch can confuse the model, leading to suboptimal performance. This formatting step is not merely cleaning but a crucial alignment of our custom data with the model's pre-existing conversational conditioning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "PREGENERATED_DATSET_NAME = \"chongcht/synthetic-creative-writing\"\n",
        "\n",
        "if use_pregenerated_dataset:\n",
        "    try:\n",
        "        dataset = load_dataset(PREGENERATED_DATSET_NAME, split=\"train\")\n",
        "        print(f\"Loaded pre-generated dataset from {PREGENERATED_DATSET_NAME}\")\n",
        "        print(f\"Dataset columns: {dataset.column_names}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load dataset from {PREGENERATED_DATSET_NAME}. Please ensure it exists and is public.\")\n",
        "        # If pre-generated dataset fails to load, we might need a fallback or to stop.\n",
        "        # For now, let's assume the synthetic generation in the previous cell will run.\n",
        "        # If you are running this cell directly, ensure use_pregenerated_dataset is False\n",
        "        # or the dataset exists and is accessible.\n",
        "\n",
        "\n",
        "# Format the dataset for Gemma 3\n",
        "# Ensure the dataset has 'prompt' and 'response' columns or adapt accordingly.\n",
        "def format_for_gemma(example):\n",
        "    prompt_text = example.get(\"prompt\", \"\") # Use .get() for safer access\n",
        "    response_text = example.get(\"response\", \"\") # Use .get() for safer access\n",
        "\n",
        "    # Handle cases where the data is already in a 'conversations' format\n",
        "    if not prompt_text and \"conversations\" in example:\n",
        "        for message in example[\"conversations\"]:\n",
        "            if message[\"role\"] == \"user\":\n",
        "                prompt_text = message[\"content\"]\n",
        "            elif message[\"role\"] == \"assistant\":\n",
        "                response_text = message[\"content\"]\n",
        "\n",
        "    if not prompt_text or not response_text:\n",
        "        # Handle cases where prompt or response might be missing\n",
        "        print(f\"Skipping example due to missing prompt or response: {example}\")\n",
        "        return None # Return None to indicate this example should be skipped\n",
        "\n",
        "    return {\n",
        "        \"conversations\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a master storyteller. Write a short, imaginative story based on the user's request. The story should be concise and suitable for a general audience.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt_text},\n",
        "            {\"role\": \"assistant\", \"content\": response_text},\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Apply the formatting function and filter out None results\n",
        "dataset = dataset.map(format_for_gemma, remove_columns=dataset.column_names).filter(lambda x: x is not None)\n",
        "\n",
        "\n",
        "print(\"Dataset formatted. Here's an example:\")\n",
        "if len(dataset) > 0:\n",
        "    print(dataset[0]['conversations'])\n",
        "else:\n",
        "    print(\"No valid examples found in the dataset after formatting.\")\n",
        "\n",
        "# Apply the chat template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma3\",\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "   convos = examples[\"conversations\"]\n",
        "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
        "   return { \"text\" : texts, }\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
        "\n",
        "print(\"\\n\\nDataset after applying chat template. Here's the first example:\")\n",
        "if len(dataset) > 0:\n",
        "    print(dataset[0]['text'])\n",
        "else:\n",
        "    print(\"No valid examples found in the dataset after applying chat template.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Fine-Tuning Core: QLoRA with Unsloth\n",
        "\n",
        "This section constitutes the core of the demonstration: the fine-tuning process itself. Here, QLoRA is employed to adapt the Gemma 3 model. The Unsloth library's `FastModel` class is central to this, automatically applying memory and speed optimizations that abstract away much of the boilerplate code.\n",
        "\n",
        "A key focus here is the **deliberate and evidence-based selection of LoRA hyperparameters**. The choices for rank (`r`), `lora_alpha`, and `target_modules` are not arbitrary; they are directly informed by empirical findings and best practices to maximize performance.\n",
        "\n",
        "| Hyperparameter     | Value             | Justification & Source                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
        "|:-------------------|:------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **r (Rank)**       | 16                | The rank determines the capacity of the LoRA adapters. `r = 16` provides a robust balance between expressive power and parameter efficiency for many tasks. While higher ranks can capture more complex patterns, they also increase memory usage and the risk of overfitting. (Source: Unsloth LoRA Guide)                                                                                                                                                                |\n",
        "| **lora_alpha**     | 32                | The alpha parameter acts as a scalar for the adapter weights. A common and effective heuristic is to set `lora_alpha = 2 * r`. This scaling gives more weight to the fine-tuned adjustments, helping the model learn the new task more effectively. (Source: Unsloth LoRA Guide)                                                                                                                                                                                           |\n",
        "| **target_modules** | All Linear Layers | This is a critical choice based on strong empirical evidence. Research shows that applying LoRA to all major linear layersâ€”encompassing both self-attention (`q_proj`, `k_proj`, `v_proj`, `o_proj`) and the feed-forward/MLP blocks (`gate_proj`, `up_proj`, `down_proj`)â€”yields performance far superior to applying it only to attention layers. This ensures the adaptation happens across the model's full representational capacity. (Source: \"LoRA Without Regret\") |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_r,\n",
        "    lora_alpha = lora_r * 2, # Heuristic: 2 * r.\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_dropout = 0.05, # A bit of regularization\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuring the Trainer\n",
        "\n",
        "The `SFTTrainer` (Supervised Fine-Tuning Trainer) requires a set of arguments to control the training process. Below is a breakdown of the key parameters used in this demonstration:\n",
        "\n",
        "*   `per_device_train_batch_size`: The number of training examples to process on each GPU per iteration. A larger batch size can lead to more stable training but requires more memory.\n",
        "*   `gradient_accumulation_steps`: This is a memory-saving technique. Instead of updating the model weights after every batch, we accumulate the gradients over multiple batches. The effective batch size becomes `per_device_train_batch_size * gradient_accumulation_steps` (in our case, 4 * 4 = 16).\n",
        "*   `warmup_steps`: The learning rate starts low and gradually increases over this number of steps. This \"warm-up\" period helps stabilize the model at the beginning of training, preventing large, disruptive updates.\n",
        "*   `max_steps`: The total number of training steps to perform. This defines the overall length of the training job.\n",
        "*   `learning_rate`: The speed at which the model learns. It's a critical hyperparameter that needs to be tuned; too high, and the model might become unstable; too low, and training will be slow.\n",
        "*   `logging_steps`: How frequently to print training metrics (like the loss) to the console.\n",
        "*   `optim`: The optimization algorithm used to update the model's weights. `adamw_8bit` is a version of the popular AdamW optimizer that uses 8-bit precision, significantly reducing memory usage.\n",
        "*   `weight_decay`: A regularization technique that penalizes large weight values, which helps prevent the model from overfitting to the training data.\n",
        "*   `lr_scheduler_type`: Defines how the learning rate changes over time. A \"linear\" scheduler decreases the learning rate linearly from its initial value to zero over the course of training.\n",
        "*   `seed`: Setting a random seed ensures that the initialization and data shuffling are the same every time you run the code, making your results reproducible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "# 1. Set up the trainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\", # This is created by apply_chat_template\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 4, # Effective batch size of 16\n",
        "        warmup_steps = 10,\n",
        "        max_steps = max_steps,\n",
        "        learning_rate = learning_rate,\n",
        "        logging_steps = 5,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to = \"none\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 2. Mask out user prompts for better performance\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<start_of_turn>user\\n\",\n",
        "    response_part = \"<start_of_turn>model\\n\",\n",
        ")\n",
        "\n",
        "# 3. Start training!\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference & Evaluation\n",
        "\n",
        "We'll now compare the performance of our fine-tuned model with the original base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# --- 1. Inference with Fine-Tuned Model ---\n",
        "prompt = \"about a lighthouse keeper who befriends a whale\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a master storyteller. Write a short, imaginative story based on the user's request. The story should be concise and suitable for a general audience.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True).removeprefix('<bos>')\n",
        "inputs = tokenizer(text, return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate output and decode, skipping the prompt\n",
        "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
        "finetuned_model_output = tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"--- Fine-Tuned Model Output ---\")\n",
        "print(finetuned_model_output)\n",
        "\n",
        "# --- 2. Inference with Base Model ---\n",
        "base_model, _ = FastModel.from_pretrained(\n",
        "    model_name = model_name, # Load the same base model for a fair comparison\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "# Generate output and decode, skipping the prompt\n",
        "outputs = base_model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
        "base_model_output = tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"\\n--- Base Model Output ---\")\n",
        "print(base_model_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation Pipeline\n",
        "\n",
        "The diagram below shows our evaluation process. We generate responses from both the original base model and our newly fine-tuned model, then pass both outputs to a more powerful \"judge\" model for an objective comparison.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    subgraph \"Input\"\n",
        "        A[Test Prompt]\n",
        "    end\n",
        "\n",
        "    subgraph \"Model Inference\"\n",
        "        A --> B[Base Gemma 3 Model];\n",
        "        A --> C[Fine-Tuned Gemma 3 Model];\n",
        "        B --> D[Output A: Base Response];\n",
        "        C --> E[Output B: Fine-Tuned Response];\n",
        "    end\n",
        "\n",
        "    subgraph \"Judgment\"\n",
        "        D --> F{\"LLM-as-a-Judge<br>(Gemini 1.5 Flash)\"};\n",
        "        E --> F;\n",
        "        F --> G[JSON Verdict & Comparison];\n",
        "    end\n",
        "\n",
        "    style F fill:#f9d77f,stroke:#333,stroke-width:2px\n",
        "    style G fill:#a2de89,stroke:#333,stroke-width:2px\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLM-as-a-Judge: The Verdict\n",
        "\n",
        "We have the outputs, but which is better? Let's ask an impartial judge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google import genai\n",
        "import json\n",
        "import os\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Ensure the Gemini API is configured for the judge model\n",
        "# This is separate from the client used for the Batch API\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    # Re-configure for non-client usage, which is simpler for single-shot generation\n",
        "    client = genai.Client(api_key=google_api_key)\n",
        "except (ImportError, KeyError):\n",
        "    if os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "         client = genai.Client(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
        "    else:\n",
        "         print(\"Please set your GOOGLE_API_KEY in Colab Secrets or as an environment variable to run the judge model.\")\n",
        "\n",
        "# Use f-strings to embed the model outputs directly into the prompt\n",
        "JUDGE_PROMPT = f\"\"\"\n",
        "You are an expert AI model evaluator. You will be given a prompt and two stories generated by two different AI models. Your task is to analyze them and determine which one is better.\n",
        "\n",
        "**Prompt:** {prompt}\n",
        "\n",
        "**Story A (from Base Model):**\n",
        "{base_model_output}\n",
        "\n",
        "**Story B (from Fine-Tuned Model):**\n",
        "{finetuned_model_output}\n",
        "\n",
        "**Evaluation Criteria:**\n",
        "1.  **Creativity:** How original, imaginative, and engaging is the story?\n",
        "2.  **Coherence:** Is the story logical, well-structured, and easy to follow?\n",
        "3.  **Adherence to Prompt:** How well does the story capture the essence of the user's request?\n",
        "\n",
        "**Instructions:**\n",
        "Provide a step-by-step analysis comparing the two stories based on the criteria above. Conclude with a final verdict in JSON format, declaring which model produced the better story and why. Provide scores from 1-10 for each criterion.\n",
        "\n",
        "**JSON Output Format:**\n",
        "```json\n",
        "{{\n",
        "  \"winner\": \"Model A\" or \"Model B\",\n",
        "  \"reasoning\": \"A brief explanation for your choice, highlighting the key differences.\",\n",
        "  \"scores\": {{\n",
        "    \"model_a\": {{ \"creativity\": X, \"coherence\": Y, \"adherence\": Z }},\n",
        "    \"model_b\": {{ \"creativity\": X, \"coherence\": Y, \"adherence\": Z }}\n",
        "  }}\n",
        "}}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    judge_model = 'gemini-2.5-flash'\n",
        "    response = client.models.generate_content(\n",
        "        model = judge_model,\n",
        "        contents = JUDGE_PROMPT,\n",
        "        config=types.GenerateContentConfig(\n",
        "            temperature=0.1\n",
        "        )\n",
        "    )\n",
        "    # show in markdown text format\n",
        "    display(Markdown(response.text))\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while calling the judge model: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4: Conclusion & Resources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving, Exporting, and Sharing on the Hugging Face Hub\n",
        "\n",
        "The final step is to save our work. This involves saving the lightweight LoRA adapters, which contain all the new knowledge learned during fine-tuning. For broader accessibility, we can also merge these adapters back into the base model and export it to various formats.\n",
        "\n",
        "**Exporting to GGUF for Local Inference**\n",
        "\n",
        "For local use with tools like LM Studio or Ollama, we convert the model to **GGUF (GPT-Generated Unified Format)**. This format is optimized for efficient CPU and GPU inference.\n",
        "\n",
        "**Pushing to the Hub**\n",
        "\n",
        "The `unsloth` library simplifies sharing on the Hugging Face Hub. To do this, we'll first save the adapters locally, then reload them to prepare for the upload. We can push multiple versions:\n",
        "1.  **LoRA Adapters:** The lightweight adapter weights alone.\n",
        "2.  **Merged 16-bit Model:** A full `float16` model, useful for cloud deployment with frameworks like VLLM.\n",
        "3.  **GGUF Model:** The quantized model for local use.\n",
        "\n",
        "This allows others to easily use your fine-tuned model in different environments. Ensure your Hugging Face token is configured."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the LoRA adapters\n",
        "model.save_pretrained(\"gemma-3-270m-creative-writer-lora\")\n",
        "tokenizer.save_pretrained(\"gemma-3-270m-creative-writer-lora\")\n",
        "\n",
        "# You can also merge the adapters and save as a full model\n",
        "# model.save_pretrained_merged(\"gemma-3-270m-creative-writer-merged\", tokenizer, save_method = \"merged_16bit\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload the model from the saved LoRA adapters to prepare for uploading.\n",
        "from unsloth import FastModel\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"gemma-3-270m-creative-writer-lora\", # Load our saved adapters\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Pushing to the Hugging Face Hub\n",
        "if hf_user != \"your-username\":\n",
        "    # Push the LoRA adapters\n",
        "    model.push_to_hub(f\"{hf_user}/gemma-3-270m-creative-writer-lora\", token = hf_token)\n",
        "    tokenizer.push_to_hub(f\"{hf_user}/gemma-3-270m-creative-writer-lora\", token = hf_token)\n",
        "\n",
        "    # Push the merged 16-bit model\n",
        "    model.push_to_hub_merged(f\"{hf_user}/gemma-3-270m-creative-writer-merged\", tokenizer, save_method = \"merged_16bit\", token = hf_token)\n",
        "\n",
        "    # Push the GGUF model\n",
        "    model.push_to_hub_gguf(f\"{hf_user}/gemma-3-270m-creative-writer-gguf\", tokenizer, token = hf_token)\n",
        "else:\n",
        "    print(\"Please set your Hugging Face username in the 'Notebook Configurations' cell to push to the Hub.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\n",
        "\n",
        "In under 30 minutes, we have:\n",
        "1.  Understood the theory behind effective LoRA configuration.\n",
        "2.  Generated a plan for a high-quality synthetic dataset.\n",
        "3.  Fine-tuned a Gemma 3 model on a custom task.\n",
        "4.  **Objectively evaluated the improvement** using an LLM-as-a-Judge.\n",
        "\n",
        "This demonstrates how accessible and powerful modern AI development can be with the right tools and techniques.\n",
        "\n",
        "### Acknowledgements & Further Reading\n",
        "*   **Unsloth:** [GitHub Repository](https://github.com/unslothai/unsloth)\n",
        "*   **Key Paper:** [LoRA Without Regret - Thinking Machines Lab](https://thinkingmachines.ai/blog/lora/)\n",
        "*   **Gemini API:** [Batch Mode Documentation](https://ai.google.dev/gemini-api/docs/batch-mode)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
