{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: Introduction & Setup\n",
        "\n",
        "### **Live Model Fine-Tuning: Customizing Gemma 3 270M in Under 30 Minutes**\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"images/devfest_waterloo.png\" alt=\"DevFest Waterloo\" style=\"width: 100%; max-width: 600px;\">\n",
        "</div>\n",
        "\n",
        "This session provides a comprehensive, live demonstration of a complete fine-tuning workflow for Google's Gemma 3 270M model, executed in under 30 minutes. The primary objective is to illustrate how recent advancements in model architecture, training techniques, and software optimization have made customizing powerful Large Language Models (LLMs) accessible to a broader audience, even in resource-constrained environments.\n",
        "\n",
        "The traditional approach, full fine-tuning, involves updating every single parameter in the model. This process is computationally intensive, requiring significant GPU memory and extended training times. This notebook will showcase a modern, efficient alternative by strategically combining three key technologies:\n",
        "\n",
        "1.  **Gemma 3 270M**: A compact, state-of-the-art open model from Google. Its small footprint makes it an ideal candidate for rapid experimentation and deployment on consumer-grade hardware.\n",
        "2.  **QLoRA (Quantized Low-Rank Adaptation)**: A highly efficient fine-tuning method. QLoRA dramatically reduces memory by quantizing the model's weights to 4-bits and then inserting small, trainable \"adapter\" layers. We only update these adapters, leaving the millions of base model parameters frozen.\n",
        "3.  **Unsloth**: An open-source library with optimized CUDA kernels that can make fine-tuning up to 2x faster while reducing memory usage by 60-70%. Unsloth is the critical enabler that makes this sub-30-minute demonstration feasible on a free Google Colab GPU.\n",
        "\n",
        "This rapid fine-tuning is possible due to the convergence of these three elements: a capable base model, a memory-frugal training technique, and a speed-optimized library.\n",
        "\n",
        "**Agenda:**\n",
        "1.  **Prerequisites:** What are Fine-Tuning and LoRA?\n",
        "2.  **Setup & Installation:** Getting our environment ready.\n",
        "3.  **Data Generation:** Creating a synthetic dataset with the Gemini Batch API.\n",
        "4.  **Live Fine-Tuning:** Loading, configuring, and training our model.\n",
        "5.  **Evaluation:** Was it worth it? A side-by-side comparison with an LLM-as-a-Judge.\n",
        "6.  **Conclusion & Resources.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Overall Fine-Tuning Pipeline\n",
        "\n",
        "This diagram illustrates the end-to-end workflow we will follow in this notebook, from generating a synthetic dataset to exporting a locally-runnable model.\n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "    subgraph \"Part 1: Data Generation\"\n",
        "        A[Start: Define Creative Themes] --> B{Gemini Batch API};\n",
        "        B --> C[Generate Synthetic Dataset];\n",
        "    end\n",
        "\n",
        "    subgraph \"Part 2: Model Training\"\n",
        "        C --> D[Load Base Gemma 3 Model];\n",
        "        D --> E[Inject QLoRA Adapters];\n",
        "        E --> F(Fine-Tune with Unsloth);\n",
        "    end\n",
        "\n",
        "    subgraph \"Part 3: Evaluation & Export\"\n",
        "        F --> G{A/B Test: Base vs. Fine-Tuned};\n",
        "        G --> H[LLM-as-a-Judge Verdict];\n",
        "        H --> I[Save LoRA Adapters];\n",
        "        I --> J[Export to GGUF for Local Inference];\n",
        "        J --> K[End: Ready for LM Studio];\n",
        "    end\n",
        "\n",
        "    style F fill:#a2de89,stroke:#333,stroke-width:2px\n",
        "    style J fill:#f9d77f,stroke:#333,stroke-width:2px\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What is Fine-Tuning?\n",
        "\n",
        "Fine-tuning adapts a general pre-trained model to a specific task using a smaller, focused dataset. It's like teaching a polymath to become a specialist in a particular field.\n",
        "\n",
        "### What is LoRA (Low-Rank Adaptation)?\n",
        "\n",
        "Full fine-tuning requires updating all model weights, which is computationally expensive. **LoRA** offers a much more efficient alternative. It freezes the original weights and injects small, trainable \"adapter\" matrices (A and B) into the model. We only train these adapters, which are a tiny fraction of the total parameters.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"images/what_is_lora.png\" alt=\"LoRA Diagram\" style=\"width: 100%; max-width: 800px;\">\n",
        "</div>\n",
        "\n",
        "This makes fine-tuning incredibly fast and memory-efficient. But does it compromise on quality?\n",
        "\n",
        "> #### **Key Insight from \"LoRA Without Regret\"**\n",
        ">\n",
        "> Research shows that LoRA can match the performance of full fine-tuning if configured correctly. One of the most critical, and often overlooked, conditions is **applying LoRA to all possible layers.**\n",
        ">\n",
        "> From the paper: *\"Attention-only LoRA significantly underperforms MLP-only LoRA, and does not further improve performance on top of LoRA-on-MLP.\"*\n",
        ">\n",
        "> This is because the MLP (feed-forward) layers contain a vast number of parameters compared to the attention layers. By ignoring them, you're leaving most of the model's knowledge untapped and creating a bottleneck for learning. **Therefore, we will target all linear layers in our configuration.**\n",
        ">\n",
        "> Link to the blog post: [LoRA Without Regret](https://thinkingmachines.ai/blog/lora/)\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"images/lora_without_regret.png\" alt=\"LoRA Diagram\" style=\"width: 100%; max-width: 800px;\">\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Dependencies\n",
        "\n",
        "Install Unsloth from GitHub for the latest patches and pin other libraries for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Install Dependencies\n",
        "# Install Unsloth from GitHub for the latest patches and pin other libraries for reproducibility.\n",
        "!pip install \"unsloth[colab-new]@git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.26\" \"trl<0.9.0\" \"peft<0.12.0\" \"accelerate<0.32.0\" \"bitsandbytes<0.44.0\"\n",
        "\n",
        "# Install the Google AI SDK for the Gemini API.\n",
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure API Keys\n",
        "\n",
        "To run this notebook, you'll need API keys for both Google Gemini and the Hugging Face Hub. For security, we'll use Colab's built-in secret manager.\n",
        "\n",
        "1.  Click on the **key icon** (ðŸ”‘) in the left-hand sidebar.\n",
        "2.  Create two new secrets:\n",
        "    *   **Name:** `GOOGLE_API_KEY` -> **Value:** Your Gemini API Key\n",
        "    *   **Name:** `HF_TOKEN` -> **Value:** Your Hugging Face access token (with `write` permissions)\n",
        "\n",
        "Once you've added these secrets, the notebook will be able to access them securely.\n",
        "<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/colab_secrets.png\" width=\"300\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title  Notebook Configurations\n",
        "#@markdown ---\n",
        "#@markdown ### **Model & Dataset**\n",
        "model_name = \"unsloth/gemma-3-270m-it\" #@param {type:\"string\"}\n",
        "max_seq_length = 2048 #@param {type:\"integer\"}\n",
        "use_pregenerated_dataset = True #@param {type:\"boolean\"}\n",
        "hf_user = \"your-username\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### **LoRA Parameters**\n",
        "#@markdown These settings control the size and complexity of the LoRA adapters.\n",
        "lora_r = 16 #@param {type:\"slider\", min:4, max:64, step:4}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### **Training Parameters**\n",
        "#@markdown Adjust these to control the training process.\n",
        "max_steps = 150 #@param {type:\"integer\"}\n",
        "learning_rate = 2e-4 #@param {type:\"number\"}\n",
        "#@markdown ---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Libraries & Configure Secrets    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core ML and Unsloth imports\n",
        "from unsloth import FastModel\n",
        "import torch\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Google Gemini API imports\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import json\n",
        "import time\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get API keys from Colab secrets\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "google_api_key = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Data is Fuel - Synthetic Data Generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bootstrapping Our Dataset with the Gemini Batch API\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"images/gemini_batch_api.png\" alt=\"Gemini Batch API\" style=\"width: 100%; max-width: 800px;\">\n",
        "</div>\n",
        "\n",
        "A common challenge in fine-tuning is the \"cold start\" problem, where you have a task in mind but lack labeled training examples. **Synthetic data generation** offers a powerful solution, allowing us to use a highly capable LLM to create a labeled dataset from scratch.\n",
        "\n",
        "This section demonstrates how to use the **Gemini Batch API** to generate our creative writing dataset. This API is specifically designed for high-volume, asynchronous tasks. It processes large numbers of requests in parallel at a 50% cost discount compared to its real-time counterpart, making it an ideal tool for large-scale data generation.\n",
        "\n",
        "The workflow is as follows:\n",
        "1. A prompt template is designed to instruct the Gemini model to generate creative stories.\n",
        "2. These prompts are formatted into a JSON Lines (JSONL) file.\n",
        "3. The file is submitted as an asynchronous batch job.\n",
        "\n",
        "Since batch jobs are not instantaneous, this demonstration will show the code for submitting the job and then proceed with a pre-generated set of results to maintain the pace of the live session. This approach showcases a sophisticated data augmentation strategy that enhances the model's generalization capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- CONFIGURATION ---\n",
        "HF_DATASET_NAME = f\"{hf_user}/gemma-3-creative-writing\"\n",
        "\n",
        "if not use_pregenerated_dataset:\n",
        "    # --- Gemini API Setup ---\n",
        "    # Initialize the client for Batch API usage\n",
        "    client = genai.Client(api_key=google_api_key)\n",
        "\n",
        "    # --- Data Generation Pipeline ---\n",
        "    SYSTEM_PROMPT = \"You are a master storyteller. Write a short, imaginative story based on the user's request. The story should be concise and suitable for a general audience. The story should not exceed 2048 tokens.\"\n",
        "\n",
        "    STORY_THEMES = [\n",
        "        \"A robot who discovers music for the first time.\",\n",
        "        \"A magical library where books come to life.\",\n",
        "        \"A detective who solves crimes in a city powered by steam.\",\n",
        "        \"Two pen pals from different planets meeting for the first time.\",\n",
        "        \"A mischievous forest spirit who plays pranks on hikers.\",\n",
        "        \"The last dragon on Earth sharing its wisdom with a young child.\",\n",
        "    ]\n",
        "\n",
        "    def create_batch_requests(themes, system_prompt):\n",
        "        \"\"\"Creates a JSONL file for the Gemini Batch API.\"\"\"\n",
        "        jsonl_file_path = 'synthetic_story_requests.jsonl'\n",
        "        with open(jsonl_file_path, 'w') as f:\n",
        "            for i, theme in enumerate(themes):\n",
        "                request = {\n",
        "                    \"key\": f\"request_{i}\",\n",
        "                    \"request\": {\n",
        "                        \"contents\": [\n",
        "                            {\"role\": \"system\", \"parts\": [{\"text\": system_prompt}]},\n",
        "                            {\"role\": \"user\", \"parts\": [{\"text\": theme}]}\n",
        "                        ],\n",
        "                        \"generation_config\": {\n",
        "                            \"max_output_tokens\": 2048,\n",
        "                            \"temperature\": 0.9,\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "                f.write(json.dumps(request) + '\\n')\n",
        "        return jsonl_file_path\n",
        "\n",
        "    # Create and run the batch job (simplified for the plan)\n",
        "    print(\"Creating batch requests file...\")\n",
        "    requests_file = create_batch_requests(STORY_THEMES * 50, SYSTEM_PROMPT) # Create 300 examples\n",
        "    print(f\"Uploading file: {requests_file}\")\n",
        "    uploaded_file = client.files.upload(file=requests_file)\n",
        "    batch_job = client.batches.create(\n",
        "        model=\"models/gemini-1.5-flash-latest\",\n",
        "        src=uploaded_file.name,\n",
        "    )\n",
        "    print(f\"Batch job created: {batch_job.name}. Polling for results...\")\n",
        "\n",
        "    # Polling logic would go here...\n",
        "    # After completion, results would be parsed and saved to a Hugging Face dataset.\n",
        "    print(\"For the demo, we will now proceed with the pre-generated dataset.\")\n",
        "    use_pregenerated_dataset = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: Live Fine-Tuning with Unsloth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True, # Use QLoRA for memory efficiency\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Formatting for Gemma 3\n",
        "\n",
        "Instruction-tuned models like Gemma 3 are pre-trained on data formatted with specific \"chat templates.\" These templates use special tokens to delineate conversational turns (e.g., from a user and a model). Adhering to this format during fine-tuning is critical for the model to correctly interpret the task and generate responses in the desired style.\n",
        "\n",
        "The Gemma 3 chat template follows this structure:\n",
        "\n",
        "`<bos><start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n{response}<end_of_turn><eos>`\n",
        "\n",
        "Failing to use the correct template introduces a **distribution shift** between our fine-tuning data and the model's original training data. This mismatch can confuse the model, leading to suboptimal performance. This formatting step is not merely cleaning but a crucial alignment of our custom data with the model's pre-existing conversational conditioning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "if use_pregenerated_dataset:\n",
        "    # This is a placeholder. You should create and upload your own dataset.\n",
        "    # If you haven't, you can use a public one like \"databricks/databricks-dolly-15k\"\n",
        "    # For this demo, we assume a dataset with 'prompt' and 'response' columns.\n",
        "    try:\n",
        "        dataset = load_dataset(HF_DATASET_NAME, split=\"train\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load dataset from {HF_DATASET_NAME}. Please ensure it exists and is public.\")\n",
        "        print(\"Using a fallback dataset for demonstration purposes.\")\n",
        "        dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:500]\")\n",
        "        # Remap columns to match our expected format\n",
        "        dataset = dataset.rename_columns({'input': 'prompt', 'output': 'response'})\n",
        "\n",
        "\n",
        "# Format the dataset for Gemma 3\n",
        "def format_for_gemma(example):\n",
        "    return {\n",
        "        \"conversations\": [\n",
        "            {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
        "            {\"role\": \"assistant\", \"content\": example[\"response\"]},\n",
        "        ]\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(format_for_gemma, remove_columns=dataset.column_names)\n",
        "\n",
        "print(\"Dataset formatted. Here's an example:\")\n",
        "print(dataset[0]['conversations'])\n",
        "\n",
        "# Apply the chat template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma3\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Fine-Tuning Core: QLoRA with Unsloth\n",
        "\n",
        "This section constitutes the core of the demonstration: the fine-tuning process itself. Here, QLoRA is employed to adapt the Gemma 3 model. The Unsloth library's `FastModel` class is central to this, automatically applying memory and speed optimizations that abstract away much of the boilerplate code.\n",
        "\n",
        "A key focus here is the **deliberate and evidence-based selection of LoRA hyperparameters**. The choices for rank (`r`), `lora_alpha`, and `target_modules` are not arbitrary; they are directly informed by empirical findings and best practices to maximize performance.\n",
        "\n",
        "| Hyperparameter     | Value             | Justification & Source                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
        "|:-------------------|:------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **r (Rank)**       | 16                | The rank determines the capacity of the LoRA adapters. `r = 16` provides a robust balance between expressive power and parameter efficiency for many tasks. While higher ranks can capture more complex patterns, they also increase memory usage and the risk of overfitting. (Source: Unsloth LoRA Guide)                                                                                                                                                                |\n",
        "| **lora_alpha**     | 32                | The alpha parameter acts as a scalar for the adapter weights. A common and effective heuristic is to set `lora_alpha = 2 * r`. This scaling gives more weight to the fine-tuned adjustments, helping the model learn the new task more effectively. (Source: Unsloth LoRA Guide)                                                                                                                                                                                           |\n",
        "| **target_modules** | All Linear Layers | This is a critical choice based on strong empirical evidence. Research shows that applying LoRA to all major linear layersâ€”encompassing both self-attention (`q_proj`, `k_proj`, `v_proj`, `o_proj`) and the feed-forward/MLP blocks (`gate_proj`, `up_proj`, `down_proj`)â€”yields performance far superior to applying it only to attention layers. This ensures the adaptation happens across the model's full representational capacity. (Source: \"LoRA Without Regret\") |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_r,\n",
        "    lora_alpha = lora_r * 2, # Heuristic: 2 * r.\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_dropout = 0.05, # A bit of regularization\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Set up the trainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\", # This is created by apply_chat_template\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 4, # Effective batch size of 16\n",
        "        warmup_steps = 10,\n",
        "        max_steps = max_steps,\n",
        "        learning_rate = learning_rate,\n",
        "        logging_steps = 5,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir=\"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 2. Mask out user prompts for better performance\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<start_of_turn>user\\n\",\n",
        "    response_part = \"<start_of_turn>model\\n\",\n",
        ")\n",
        "\n",
        "# 3. Start training!\n",
        "trainer_stats = trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 3.5: Inference & Evaluation\n",
        "import torch\n",
        "\n",
        "# --- 1. Inference with Fine-Tuned Model ---\n",
        "prompt = \"Write a short story about a lighthouse keeper who befriends a whale\"\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "text = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True).removeprefix('<bos>')\n",
        "inputs = tokenizer(text, return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate output and decode, skipping the prompt\n",
        "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
        "finetuned_model_output = tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"--- Fine-Tuned Model Output ---\")\n",
        "print(finetuned_model_output)\n",
        "\n",
        "\n",
        "# --- 2. Inference with Base Model ---\n",
        "# To save VRAM, we'll clear the fine-tuned model before loading the base model.\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "base_model, _ = FastModel.from_pretrained(\n",
        "    model_name = model_name, # Load the same base model for a fair comparison\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "# Generate output and decode, skipping the prompt\n",
        "outputs = base_model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
        "base_model_output = tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"\\n--- Base Model Output ---\")\n",
        "print(base_model_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation Pipeline\n",
        "\n",
        "The diagram below shows our evaluation process. We generate responses from both the original base model and our newly fine-tuned model, then pass both outputs to a more powerful \"judge\" model for an objective comparison.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    subgraph \"Input\"\n",
        "        A[Test Prompt]\n",
        "    end\n",
        "\n",
        "    subgraph \"Model Inference\"\n",
        "        A --> B[Base Gemma 3 Model];\n",
        "        A --> C[Fine-Tuned Gemma 3 Model];\n",
        "        B --> D[Output A: Base Response];\n",
        "        C --> E[Output B: Fine-Tuned Response];\n",
        "    end\n",
        "\n",
        "    subgraph \"Judgment\"\n",
        "        D --> F{\"LLM-as-a-Judge<br>(Gemini 1.5 Flash)\"};\n",
        "        E --> F;\n",
        "        F --> G[JSON Verdict & Comparison];\n",
        "    end\n",
        "\n",
        "    style F fill:#f9d77f,stroke:#333,stroke-width:2px\n",
        "    style G fill:#a2de89,stroke:#333,stroke-width:2px\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLM-as-a-Judge: The Verdict\n",
        "\n",
        "We have the outputs, but which is better? Let's ask an impartial judge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Ensure the Gemini API is configured for the judge model\n",
        "# This is separate from the client used for the Batch API\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    # Re-configure for non-client usage, which is simpler for single-shot generation\n",
        "    genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "except (ImportError, KeyError):\n",
        "    if os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "         genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
        "    else:\n",
        "         print(\"Please set your GOOGLE_API_KEY in Colab Secrets or as an environment variable to run the judge model.\")\n",
        "\n",
        "# Use f-strings to embed the model outputs directly into the prompt\n",
        "JUDGE_PROMPT = f\"\"\"\n",
        "You are an expert AI model evaluator. You will be given a prompt and two stories generated by two different AI models. Your task is to analyze them and determine which one is better.\n",
        "\n",
        "**Prompt:** {prompt}\n",
        "\n",
        "**Story A (from Base Model):**\n",
        "{base_model_output}\n",
        "\n",
        "**Story B (from Fine-Tuned Model):**\n",
        "{finetuned_model_output}\n",
        "\n",
        "**Evaluation Criteria:**\n",
        "1.  **Creativity:** How original, imaginative, and engaging is the story?\n",
        "2.  **Coherence:** Is the story logical, well-structured, and easy to follow?\n",
        "3.  **Adherence to Prompt:** How well does the story capture the essence of the user's request?\n",
        "\n",
        "**Instructions:**\n",
        "Provide a step-by-step analysis comparing the two stories based on the criteria above. Conclude with a final verdict in JSON format, declaring which model produced the better story and why. Provide scores from 1-10 for each criterion.\n",
        "\n",
        "**JSON Output Format:**\n",
        "```json\n",
        "{{\n",
        "  \"winner\": \"Model A\" or \"Model B\",\n",
        "  \"reasoning\": \"A brief explanation for your choice, highlighting the key differences.\",\n",
        "  \"scores\": {{\n",
        "    \"model_a\": {{ \"creativity\": X, \"coherence\": Y, \"adherence\": Z }},\n",
        "    \"model_b\": {{ \"creativity\": X, \"coherence\": Y, \"adherence\": Z }}\n",
        "  }}\n",
        "}}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    judge_model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "    response = judge_model.generate_content(JUDGE_PROMPT)\n",
        "    print(response.text)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while calling the judge model: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4: Conclusion & Resources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving the Model Adapters\n",
        "The final step is to save the trained LoRA adapters. These adapters are lightweight, typically only a few megabytes in size, which underscores the efficiency of the PEFT methodology. They contain all the new knowledge learned during fine-tuning and can be easily stored, shared, and loaded on top of the base Gemma 3 model for future inference tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the LoRA adapters\n",
        "model.save_pretrained(\"gemma-3-creative-writer-lora\")\n",
        "tokenizer.save_pretrained(\"gemma-3-creative-writer-lora\")\n",
        "\n",
        "# You can also merge the adapters and save as a full model\n",
        "# model.save_pretrained_merged(\"gemma-3-creative-writer-merged\", tokenizer, save_method = \"merged_16bit\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exporting to GGUF for Local Inference\n",
        "\n",
        "To run our fine-tuned model on a local machine using tools like LM Studio or Ollama, we need to convert it to the **GGUF (GPT-Generated Unified Format)**. This format is highly optimized for running efficiently on CPUs and various GPUs, making it the standard for local LLM inference.\n",
        "\n",
        "The process involves two steps:\n",
        "1.  **Merge Adapters:** First, we merge the trained LoRA adapters back into the base model to create a full, fine-tuned model.\n",
        "2.  **Save as GGUF:** We then save this merged model in the GGUF format. Unsloth simplifies this with a built-in `save_pretrained_gguf` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, we need to reload the fine-tuned model.\n",
        "# The previous cell deleted the model to save VRAM for the base model comparison.\n",
        "from unsloth import FastModel\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"gemma-3-creative-writer-lora\", # Load our saved adapters\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "# Merge the LoRA adapters into the base model.\n",
        "# This creates a new, full model with our fine-tuned weights.\n",
        "model.save_pretrained_merged(\"gemma-3-creative-writer-merged\", tokenizer, save_method = \"merged_16bit\",)\n",
        "\n",
        "# Now, save the merged model in GGUF format.\n",
        "model.save_pretrained_gguf(\"gemma-3-creative-writer-gguf\", tokenizer)\n",
        "\n",
        "print(\"Model successfully converted to GGUF format.\")\n",
        "print(\"You can now find the GGUF file in the 'gemma-3-creative-writer-gguf' directory.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\n",
        "\n",
        "In under 30 minutes, we have:\n",
        "1.  Understood the theory behind effective LoRA configuration.\n",
        "2.  Generated a plan for a high-quality synthetic dataset.\n",
        "3.  Fine-tuned a Gemma 3 model on a custom task.\n",
        "4.  **Objectively evaluated the improvement** using an LLM-as-a-Judge.\n",
        "\n",
        "This demonstrates how accessible and powerful modern AI development can be with the right tools and techniques.\n",
        "\n",
        "### Acknowledgements & Further Reading\n",
        "*   **Unsloth:** [GitHub Repository](https://github.com/unslothai/unsloth)\n",
        "*   **Key Paper:** [LoRA Without Regret - Thinking Machines Lab](https://thinkingmachines.ai/blog/lora/)\n",
        "*   **Gemini API:** [Batch Mode Documentation](https://ai.google.dev/gemini-api/docs/batch-mode)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
